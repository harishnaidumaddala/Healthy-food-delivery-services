# M6 - Healthy Food Delivery services dataset - Programming Data Visualizations that Show Relationships in Python
# Created by Harish Naidu Maddala, Srinadh kocherla(Team_5).
# Created for Assignment_6
# Created on October 27, 2023

#========================SECTION SEPARATOR

# All lines beginning with the # symbol are comments and therefore will not be treated as executable code.


#========================SECTION SEPARATOR


# Install packages if required
if(!require("rmarkdown")) install.packages("rmarkdown") # An if statement is used for checking the condition (!) that a set of code will require that a package called "rmarkdown" is installed for working with .Rmd files and if so using the install.packages() function to install the package.
if(!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE) # For installing a package called "ggplot2" for plotting and to ensure that it also installs other things that ggplot2 needs (dependencies).
if(!require("dplyr")) install.packages("dplyr") # # An if statement is used for checking the condition (!) that a set of code will require that a package called "dplyr" is installed for data manipulation and if so using the install.packages() function to install the package.
if(!require("tidyverse")) install.packages("tidyverse") # For installing a package called "tidyverse" for data processing.
if(!require("scales")) install.packages("scales") # For installing a package called "scales" for creating chart axes labels and legends (internal scaling infrastructure of visualizations).
if(!require("GGally")) install.packages("GGally") # For installing a package called "GGally" for creating parallel coordinates.
if(!require("assertive")) install.packages("assertive") # For installing a package called "assertive" for iteratively checking R code integrity.
if(!require("testthat")) install.packages("testthat") # For installing a package called "testthat" for iteratively checking our R code for errors.
if(!require("remotes")) install.packages("remotes") # For installing a package called "remotes" to allow us to grab things that are hosted at a url.
remotes::install_github("ricardo-bion/ggradar", force=TRUE) # Using a package called remotes for force-installing a package called "ggradar" from a developer's github.
if(!require("fmsb")) install.packages("fmsb") # For installing a package called "fmsb" for creating radar charts.
if(!require("gridExtra")) install.packages("gridExtra") # For installing a package called "gridExtra" for use in creating scatter plots.
if(!require("stringr")) install.packages("stringr") # For installing a package called "stringr" for use in creating scatter plots.
if(!require("data.table")) install.packages("data.table") # For installing a package called "data.table" for creating pivot tables.
if(!require("readxl"))install.packages("readxl") #to read excel file
install.packages("plotly") # to create interactive parallel coordinate plots. 
# Some of these packages may not have been used in the final version of this file.

#========================SECTION SEPARATOR

# Load required libraries

library("rmarkdown") # The library() function is used to load a package called "rmarkdown" for working with .Rmd files.
library("ggplot2") # The library() function is used to load a package called "ggplot2" for plotting.
library("dplyr") # The library() function is used to load a package called "dplyr" for data manipulation.
library(tidyverse) # For loading a library called "tidyverse" for data processing.
library(scales) # For loading a library called "scales" for creating chart axes labels and legends (internal scaling infrastructure of visualizations).
library(GGally) # For loading a library called "GGally" for creating parallel coordinates.
library(ggradar) # For loading a library called "ggradar" for creating radar charts.
library(fmsb) # For loading a library called "fmsb" for creating radar charts.
library(testthat) # For loading a library called "testthat" for iteratively checking our R code for errors.
library(remotes) # For loading a library called "remotes" to allow us to grab things that are hosted at a url.
library(assertive, warn.conflicts = FALSE) # For loading a library called "assertive" to iteratively check R code integrity, but to prevent it from telling you each time there might be a conflict between libraries or packages.
library(gridExtra) # For loading a library called "gridExtra" for use in creating scatter plots.
library(stringr) # For loading a library called "stringr" for use in creating scatter plots.
library(data.table) # For loading a library called "data.table" for creating pivot tables.
library(readxl) # to load readxl library
library(plotly) # to create interactive parallel coordinate plots load plotly. 

# WARNING about this next code line: When this line runs, you get a prompt at the bottom of your screen within the Console pane saying "Would you like to use a personal library instead? (yes/No/cancel) ". When this happens, position your cursor down in the Console after the prompt and type "yes" and then press enter/return on your keyboard.
update.packages(ask = FALSE) # Use the update.package() function to designate an argument to prevent being asked if you want to update each package one at a time.

#========================SECTION SEPARATOR

# Defining the file paths
M1 <- "5 - Healthy Food Delivery - M1 - Initial dataset.xlsx"
M6 <- "5 - Healthy Food Delivery - M6 - Relationships.xlsx"

# Read data from the files.
data_M1 <- read_excel(M1)
data_M6 <- read_excel(M6)

#========================SECTION SEPARATOR

# Use the head() function to show the first 6 rows of a dataset that was assigned to a variable called "data".
head(data_M1)

#========================SECTION SEPARATOR

# Merge the two data frames based on a common column "ID"
M1_M6 <- merge(data_M1, data_M6, by = "ID")

# Selecting specific columns and creating a new data frame
selected_columns <- c('Q2', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26')
new_M1_M6 <- M1_M6[selected_columns]

# Renaming the columns
colnames(new_M1_M6) <- c('Ethnicity','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday')

new_M1_M6

#========================SECTION SEPARATOR

# Mapping values for Ethnicity column (Q2)
ethnicity_mapping <- c(
  `1` = 'Ameridan Indian',
  `2` = 'African American',
  `3` = 'Mexican American',
  `4` = 'Other Hispanic',
  `5` = 'Asian',
  `6` = 'Southeast Asian',
  `7` = 'Pacific Islander',
  `8` = 'Filipino',
  `9` = 'Multiple Ethnicities',
  `10` = 'White',
  `11` = 'Other/Not Stated',
  `12` = 'International'
)

# Mapping values for Q20, Q21, Q22, Q23, Q24 , Q25, Q26 columns
likelihood_mapping <- c(
  `-3` = 'I will not use service',
  `-2` = 'Highly Unlikely',
  `-1` = 'Unlikely',
  `0` = 'Unsure',
  `1` = 'Likely',
  `2` = 'Highly Likely'
)

# Mapping Ethnicity column and Q20, Q21, Q22, Q23, Q24, Q25, Q26 columns
new_M1_M6 <- new_M1_M6 %>%
  mutate(Ethnicity = recode(Ethnicity, !!!ethnicity_mapping),
         Monday = recode(Monday, !!!likelihood_mapping),
         Tuesday = recode(Tuesday, !!!likelihood_mapping),
         Wednesday = recode(Wednesday, !!!likelihood_mapping),
         Thursday = recode(Thursday, !!!likelihood_mapping),
         Friday = recode(Friday, !!!likelihood_mapping),
         Saturday = recode(Saturday, !!!likelihood_mapping),
         Sunday = recode(Sunday, !!!likelihood_mapping)
         )

new_M1_M6

# Group by 'Ethnicity' and calculate the most common visit type for each group
Common_visit_type_by_Ethnicity <- new_M1_M6 %>%
  group_by(Ethnicity) %>%
  summarise(Monday = names(sort(table(Monday), decreasing = TRUE)[1]),
            Tuesday = names(sort(table(Tuesday), decreasing = TRUE)[1]),
            Wednesday = names(sort(table(Wednesday), decreasing = TRUE)[1]),
            Thursday = names(sort(table(Thursday), decreasing = TRUE)[1]),
            Friday = names(sort(table(Friday), decreasing = TRUE)[1]),
            Saturday = names(sort(table(Saturday), decreasing = TRUE)[1]),
            Sunday = names(sort(table(Sunday), decreasing = TRUE)[1]),
            )



# Print the resulting DataFrame
print(Common_visit_type_by_Ethnicity)

#========================SECTION SEPARATOR

# Gather columns into key-value pairs
data_long <- Common_visit_type_by_Ethnicity %>%
  gather(key = "Visit_Type", value = "Visit_Status", -Ethnicity)

# Reshape the data to long format using tidyr
melted_data <- Common_visit_type_by_Ethnicity %>%
  gather(key = "Visit_Type", value = "Likelihood", 
         Monday, Tuesday, Wednesday, Thursday, Friday,Saturday,Sunday)

# Reorder the levels of Visit_Type column for proper plotting
melted_data$Visit_Type <- factor(melted_data$Visit_Type, 
                                 levels = c("Monday", 
                                            "Tuesday", "Wednesday","Thursday", "Friday","Saturday", "Sunday"))

# Create the parallel coordinates plot
parallel_plot <- ggplot(data = melted_data, aes(x = Visit_Type, y = Likelihood, color = Ethnicity, group = Ethnicity)) +
  geom_line(size = 1) +
  theme_minimal() +
  labs(title = "Likelihood of using Healthy food Delivery services during Weekdays based on Ethnicity",
       y = "Likelihood",
       x = "Weekdays",
       color = "Ethnicity") +
  theme(plot.title = element_text(size = 16),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        legend.title = element_text(size = 14),
        legend.text = element_text(size = 12))

# Print the plot
print(parallel_plot)
